{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n!pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U pyarrow \"datasets==4.3.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:23:23.741920Z","iopub.execute_input":"2025-12-14T06:23:23.742372Z","iopub.status.idle":"2025-12-14T06:23:27.627976Z","shell.execute_reply.started":"2025-12-14T06:23:23.742351Z","shell.execute_reply":"2025-12-14T06:23:27.627191Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (22.0.0)\nRequirement already satisfied: datasets==4.3.0 in /usr/local/lib/python3.11/dist-packages (4.3.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==4.3.0) (3.20.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==4.3.0) (1.26.4)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==4.3.0) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==4.3.0) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==4.3.0) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==4.3.0) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==4.3.0) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==4.3.0) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==4.3.0) (0.70.16)\nRequirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (2025.9.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets==4.3.0) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==4.3.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==4.3.0) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets==4.3.0) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets==4.3.0) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets==4.3.0) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets==4.3.0) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets==4.3.0) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets==4.3.0) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets==4.3.0) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==4.3.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==4.3.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==4.3.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==4.3.0) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==4.3.0) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==4.3.0) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==4.3.0) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==4.3.0) (2.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==4.3.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==4.3.0) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==4.3.0) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==4.3.0) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets==4.3.0) (1.3.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets==4.3.0) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets==4.3.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets==4.3.0) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets==4.3.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets==4.3.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets==4.3.0) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom unsloth import FastLanguageModel\n\nprint(\"Torch:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"Unsloth import OK ‚úÖ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:23:54.137712Z","iopub.execute_input":"2025-12-14T06:23:54.137946Z","iopub.status.idle":"2025-12-14T06:23:54.142664Z","shell.execute_reply.started":"2025-12-14T06:23:54.137928Z","shell.execute_reply":"2025-12-14T06:23:54.141733Z"}},"outputs":[{"name":"stdout","text":"Torch: 2.6.0+cu124\nCUDA available: True\nUnsloth import OK ‚úÖ\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from datasets import load_dataset\nimport re\n\nds = load_dataset(\"namnguyenba2003/QA-law-dataset\", split=\"train\")  # 60k rows :contentReference[oaicite:1]{index=1}\n\n# --- 1) Keyword whitelist (match th√¨ GI·ªÆ) ---\nTRAFFIC = r\"\"\"\ngiao\\s*th√¥ng|ƒë∆∞·ªùng\\s*b·ªô|tr·∫≠t\\s*t·ª±\\s*an\\s*to√†n\\s*giao\\s*th√¥ng|\nxe\\s*m√°y|m√¥\\s*t√¥|√¥\\s*t√¥|xe\\s*ƒë·∫°p|xe\\s*t·∫£i|xe\\s*kh√°ch|\nGPLX|gi·∫•y\\s*ph√©p\\s*l√°i\\s*xe|b·∫±ng\\s*l√°i|\nv∆∞·ª£t\\s*ƒë√®n\\s*ƒë·ªè|ƒëi\\s*sai\\s*l√†n|l·∫•n\\s*l√†n|v∆∞·ª£t\\s*qu√°\\s*t·ªëc\\s*ƒë·ªô|qu√°\\s*t·ªëc\\s*ƒë·ªô|\nn·ªìng\\s*ƒë·ªô\\s*c·ªìn|ma\\s*t√∫y|c·ªìn|\nm≈©\\s*b·∫£o\\s*hi·ªÉm|bi·ªÉn\\s*b√°o|v·∫°ch\\s*k·∫ª\\s*ƒë∆∞·ªùng|ƒë√®n\\s*t√≠n\\s*hi·ªáu|\nC·∫£nh\\s*s√°t\\s*giao\\s*th√¥ng|CSGT|\nNgh·ªã\\s*ƒë·ªãnh\\s*100/2019|Ngh·ªã\\s*ƒë·ªãnh\\s*123/2021|Ngh·ªã\\s*ƒë·ªãnh\\s*168/2024|\ntr·ª´\\s*ƒëi·ªÉm|ph·ª•c\\s*h·ªìi\\s*ƒëi·ªÉm\n\"\"\"\n\nCIVIL = r\"\"\"\nb·ªô\\s*lu·∫≠t\\s*d√¢n\\s*s·ª±|BLDS|\nh·ª£p\\s*ƒë·ªìng|giao\\s*k·∫øt|vi\\s*ph·∫°m\\s*h·ª£p\\s*ƒë·ªìng|h·ªßy\\s*h·ª£p\\s*ƒë·ªìng|ƒë∆°n\\s*ph∆∞∆°ng|\nƒë·∫∑t\\s*c·ªçc|ph·∫°t\\s*vi\\s*ph·∫°m|b·ªìi\\s*th∆∞·ªùng|thi·ªát\\s*h·∫°i|tr√°ch\\s*nhi·ªám\\s*b·ªìi\\s*th∆∞·ªùng|\nƒë√≤i\\s*n·ª£|nghƒ©a\\s*v·ª•\\s*tr·∫£\\s*n·ª£|l√£i\\s*su·∫•t|l√£i\\s*ch·∫≠m\\s*tr·∫£|\nchuy·ªÉn\\s*nh∆∞·ª£ng|t·∫∑ng\\s*cho|th·ª´a\\s*k·∫ø|di\\s*ch√∫c|\nquy·ªÅn\\s*s·ªü\\s*h·ªØu|chi·∫øm\\s*h·ªØu|t√†i\\s*s·∫£n|mua\\s*b√°n|vay|m∆∞·ª£n|\ntranh\\s*ch·∫•p\\s*d√¢n\\s*s·ª±\n\"\"\"\n\n# --- 2) Blacklist (match th√¨ LO·∫†I) ƒë·ªÉ tr√°nh k√©o v√†o c√°c m·∫£ng kh√°c trong dataset ---\nBLACKLIST = r\"\"\"\nh√†ng\\s*kh√¥ng|s√¢n\\s*bay|h·∫£i\\s*quan|thu·∫ø|ch·ª©ng\\s*kho√°n|x√¢y\\s*d·ª±ng|ƒë·∫•t\\s*ƒëai|\nlao\\s*ƒë·ªông|bhxh|b·∫£o\\s*hi·ªÉm\\s*x√£\\s*h·ªôi|h√¨nh\\s*s·ª±|t·ªë\\s*t·ª•ng\\s*h√¨nh\\s*s·ª±|\ndoanh\\s*nghi·ªáp|ƒë·∫ßu\\s*t∆∞|th∆∞∆°ng\\s*m·∫°i|s·ªü\\s*h·ªØu\\s*tr√≠\\s*tu·ªá|y\\s*t·∫ø\n\"\"\"\n\ntraffic_pat = re.compile(TRAFFIC, re.I | re.X)\ncivil_pat   = re.compile(CIVIL,   re.I | re.X)\nblack_pat   = re.compile(BLACKLIST, re.I | re.X)\n\ndef keep(x):\n    q = (x.get(\"question\") or \"\")\n    a = (x.get(\"answer\") or \"\")\n    u = (x.get(\"url\") or \"\")\n    text = f\"{q}\\n{a}\\n{u}\"\n\n    # whitelist hit?\n    hit = bool(traffic_pat.search(text) or civil_pat.search(text))\n    if not hit:\n        return False\n\n    # blacklist: n·∫øu d√≠nh qu√° r√µ ngo√†i ng√†nh th√¨ lo·∫°i\n    if black_pat.search(text):\n        # v·∫´n cho ph√©p ‚Äúngo·∫°i l·ªá‚Äù n·∫øu n√≥ c≈©ng d√≠nh r·∫•t m·∫°nh giao th√¥ng/d√¢n s·ª±\n        if not (traffic_pat.search(text) or civil_pat.search(text)):\n            return False\n\n    return True\n\nfiltered = ds.filter(keep)\n\nprint(\"Total:\", len(ds), \"Filtered:\", len(filtered))\n\n# (khuy√™n) dedup theo question ƒë·ªÉ gi·∫£m tr√πng\nseen = set()\ndef dedup(ex):\n    q = (ex[\"question\"] or \"\").strip().lower()\n    if q in seen: \n        return False\n    seen.add(q)\n    return True\n\nfiltered = filtered.filter(dedup)\nprint(\"After dedup:\", len(filtered))\n\n# L∆∞u ra file ƒë·ªÉ train nhanh\n#filtered.to_json(\"qa_traffic_civil.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:25:11.279395Z","iopub.execute_input":"2025-12-14T06:25:11.279707Z","iopub.status.idle":"2025-12-14T06:26:28.813175Z","shell.execute_reply.started":"2025-12-14T06:25:11.279685Z","shell.execute_reply":"2025-12-14T06:26:28.812415Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/384 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f3a0eae37934b9aacc129208c415ab0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4961061c49a4ad98d44742ee6144171"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/60000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2d35163d28c4ef4bc25014f60cadb48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/60000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6da31a90dafa478bb1bbec8d4fad44dc"}},"metadata":{}},{"name":"stdout","text":"Total: 60000 Filtered: 21260\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/21260 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db4e25e862674a4b94cadb7949ce104a"}},"metadata":{}},{"name":"stdout","text":"After dedup: 21248\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\nmax_seq_length = 1024\ndtype = None\nmodel_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = model_name,\n    dtype = dtype, # None for auto detection\n    max_seq_length = max_seq_length, # Choose any for long context!\n    load_in_4bit = True,  # 4 bit quantization to reduce memory\n    full_finetuning = False, # [NEW!] We have full finetuning now!\n    # token = \"hf_...\", # use one if using gated models\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:26:55.309538Z","iopub.execute_input":"2025-12-14T06:26:55.309847Z","iopub.status.idle":"2025-12-14T06:27:09.657220Z","shell.execute_reply.started":"2025-12-14T06:26:55.309824Z","shell.execute_reply":"2025-12-14T06:27:09.656582Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.12.5: Fast Llama patching. Transformers: 4.57.3.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd4a1f52490e40e79bb210b559acd605"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c6cb2da15f546a892d31e721155a9d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"329cf6f41b7345669bc821694f3fc392"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de45cf4e30cd44b3b68f89911dc848de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e2834a29879493fadf9db728c22a4e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5537bde401fa4674b4ab925854e22920"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:27:09.658272Z","iopub.execute_input":"2025-12-14T06:27:09.658513Z","iopub.status.idle":"2025-12-14T06:27:15.505951Z","shell.execute_reply.started":"2025-12-14T06:27:09.658494Z","shell.execute_reply":"2025-12-14T06:27:15.505121Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.12.5 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"messages = [\n    {'role': 'system', 'content': 'B·∫°n l√† chuy√™n gia gi√†u kinh nghi·ªám trong lƒ©nh v·ª±c d√¢n s·ª± v√† r·∫•t am hi·ªÉu v·ªÅ lu·∫≠t giao th√¥ng', 'thinking': None},\n    {\"role\": \"user\", \"content\": \"‚ÄúD√πng b·∫±ng l√°i xe √¥ t√¥ gi·∫£ b·ªã ph·∫°t bao nhi√™u 2025? \"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt = True,\n    return_tensors = \"pt\",\n    return_dict = True,\n    reasoning_effort = \"low\",\n).to(model.device)\nfrom transformers import TextStreamer\n_ = model.generate(**inputs, max_new_tokens = 512, streamer = TextStreamer(tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:27:15.506818Z","iopub.execute_input":"2025-12-14T06:27:15.507123Z","iopub.status.idle":"2025-12-14T06:27:26.716310Z","shell.execute_reply.started":"2025-12-14T06:27:15.507103Z","shell.execute_reply":"2025-12-14T06:27:26.715715Z"}},"outputs":[{"name":"stdout","text":"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 14 Dec 2025\n\nB·∫°n l√† chuy√™n gia gi√†u kinh nghi·ªám trong lƒ©nh v·ª±c d√¢n s·ª± v√† r·∫•t am hi·ªÉu v·ªÅ lu·∫≠t giao th√¥ng<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n‚ÄúD√πng b·∫±ng l√°i xe √¥ t√¥ gi·∫£ b·ªã ph·∫°t bao nhi√™u 2025?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nTheo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t Vi·ªát Nam, s·ª≠ d·ª•ng b·∫±ng l√°i xe √¥ t√¥ gi·∫£ l√† h√†nh vi vi ph·∫°m ph√°p lu·∫≠t v·ªÅ giao th√¥ng ƒë∆∞·ªùng b·ªô.\n\nTheo ƒêi·ªÅu 4 Ngh·ªã ƒë·ªãnh 100/2019/Nƒê-CP, ng∆∞·ªùi s·ª≠ d·ª•ng b·∫±ng l√°i xe √¥ t√¥ gi·∫£ b·ªã x·ª≠ ph·∫°t t·ª´ 1 tri·ªáu ƒë·ªìng ƒë·∫øn 2 tri·ªáu ƒë·ªìng.<|eot_id|>\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"filtered","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:27:40.025990Z","iopub.execute_input":"2025-12-14T06:27:40.026774Z","iopub.status.idle":"2025-12-14T06:27:40.031563Z","shell.execute_reply.started":"2025-12-14T06:27:40.026746Z","shell.execute_reply":"2025-12-14T06:27:40.030994Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['url', 'time', 'question', 'answer'],\n    num_rows: 21248\n})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def formatting_prompts_func(examples):\n    convos = examples[\"messages\"]\n    texts = [\n        tokenizer.apply_chat_template(\n            convo,\n            tokenize=False,\n            add_generation_prompt=False,\n        )\n        for convo in convos\n    ]\n    return {\"text\": texts}\n\n\nfrom datasets import load_dataset\n\n# Login using e.g. `huggingface-cli login` to access this dataset\ndataset = load_dataset(\"namnguyenba2003/QA-law-dataset\", split=\"train\")\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:27:43.282099Z","iopub.execute_input":"2025-12-14T06:27:43.282636Z","iopub.status.idle":"2025-12-14T06:27:43.849704Z","shell.execute_reply.started":"2025-12-14T06:27:43.282613Z","shell.execute_reply":"2025-12-14T06:27:43.849099Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['url', 'time', 'question', 'answer'],\n    num_rows: 60000\n})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"SYSTEM_PROMPT = (\n    \"B·∫°n l√† tr·ª£ l√Ω ph√°p l√Ω ti·∫øng Vi·ªát, chuy√™n tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam \"\n    \"ng·∫Øn g·ªçn, ch√≠nh x√°c v√† d·ªÖ hi·ªÉu.\"\n)\n\ndef to_messages(example):\n    q = \"\" if example[\"question\"] is None else str(example[\"question\"])\n    a = \"\" if example[\"answer\"] is None else str(example[\"answer\"])\n    return {\n        \"messages\": [\n            {\"role\": \"system\",    \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\",      \"content\": q},\n            {\"role\": \"assistant\", \"content\": a},\n        ]\n    }\n\nfiltered = filtered.filter(\n    lambda ex: ex[\"question\"] is not None and ex[\"answer\"] is not None\n)\n\nfiltered = filtered.map(\n    to_messages,\n    remove_columns=[\"url\", \"time\", \"question\", \"answer\"],\n)\nprocessed_dataset = filtered.map(formatting_prompts_func, batched=True)\n\nprint(processed_dataset)\nprint(processed_dataset[0][\"text\"][:500])  # xem th·ª≠ 1 m·∫´u","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:27:43.850798Z","iopub.execute_input":"2025-12-14T06:27:43.851132Z","iopub.status.idle":"2025-12-14T06:27:49.046476Z","shell.execute_reply.started":"2025-12-14T06:27:43.851112Z","shell.execute_reply":"2025-12-14T06:27:49.045715Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/21248 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5df81102eef4c00af02e4c59c2443e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21248 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c91466fbaeba462fa35d5d7cf45b5b51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21248 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61afa3d25b7a4b7b812a774be1caae13"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['messages', 'text'],\n    num_rows: 21248\n})\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 14 Dec 2025\n\nB·∫°n l√† tr·ª£ l√Ω ph√°p l√Ω ti·∫øng Vi·ªát, chuy√™n tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam ng·∫Øn g·ªçn, ch√≠nh x√°c v√† d·ªÖ hi·ªÉu.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nNhi·ªám v·ª•, gi·∫£i ph√°p th·ª±c hi·ªán m·ª•c ti√™u k·∫ø ho·∫°ch chuy·ªÉn ƒë·ªïi s·ªë c·ªßa B·ªô X√¢y d·ª±ng nƒÉm 2025 l√† g√¨?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nN·ªôi dung n√†y ƒë∆∞·ª£c quy ƒë·ªãnh t·∫°i M·ª•c 2 K·∫ø ho·∫°ch ban h√†nh k√®m theoQ\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from trl import SFTConfig, SFTTrainer\nfrom transformers import EarlyStoppingCallback\n\nearly_stop = EarlyStoppingCallback(\n    early_stopping_patience = 3,   # sau 3 l·∫ßn eval loss kh√¥ng c·∫£i thi·ªán th√¨ d·ª´ng\n    early_stopping_threshold = 0.0 # c·∫£i thi·ªán ph·∫£i < 0.0 m·ªõi t√≠nh l√† kh√¥ng t·ªët h∆°n\n)\n\nnew_dataset = processed_dataset.train_test_split(\n    test_size = 0.01,  # 1% l√†m dev/eval\n    shuffle   = True,\n    seed      = 3407,\n)\n\ntrain_dataset = new_dataset[\"train\"]\neval_dataset  = new_dataset[\"test\"]\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset,\n    eval_dataset = eval_dataset,\n    args = SFTConfig(\n        per_device_train_batch_size = 1,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        num_train_epochs = 1, # Set this for 1 full training run.\n        # max_steps = 30,\n        learning_rate = 2e-4,\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\", # Use this for WandB etc\n        \n        eval_strategy = \"steps\", \n        eval_steps = 100,\n        save_strategy = \"steps\",\n        save_steps = 100,\n        load_best_model_at_end = True,\n        metric_for_best_model = \"eval_loss\",\n        greater_is_better = False,\n        \n        \n    ),\n    callbacks = [early_stop],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:27:56.019459Z","iopub.execute_input":"2025-12-14T06:27:56.020112Z","iopub.status.idle":"2025-12-14T06:28:18.700367Z","shell.execute_reply.started":"2025-12-14T06:27:56.020087Z","shell.execute_reply":"2025-12-14T06:28:18.699522Z"}},"outputs":[{"name":"stderr","text":"[trl.trainer.sft_trainer|WARNING]You are using a per_device_train_batch_size of 1 with padding-free training. Using a batch size of 1 anihilate the benefits of padding-free training. Please consider increasing the batch size to at least 2.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/21035 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6083e1fd7bd6449aa85eafb85675c21f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/213 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eeda060979b4a78ba26a896131477d4"}},"metadata":{}},{"name":"stdout","text":"ü¶• Unsloth: Padding-free auto-enabled, enabling faster training.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:28:34.081712Z","iopub.execute_input":"2025-12-14T06:28:34.082260Z","iopub.status.idle":"2025-12-14T15:18:45.502725Z","shell.execute_reply.started":"2025-12-14T06:28:34.082223Z","shell.execute_reply":"2025-12-14T15:18:45.501983Z"}},"outputs":[{"name":"stderr","text":"The model is already on multiple devices. Skipping the move to device specified in `args`.\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 21,035 | Num Epochs = 1 | Total steps = 5,259\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n \"-____-\"     Trainable parameters = 12,156,928 of 3,224,906,752 (0.38% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5259' max='5259' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5259/5259 8:50:01, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.217000</td>\n      <td>1.151952</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.214300</td>\n      <td>1.104192</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.095000</td>\n      <td>1.077592</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.023600</td>\n      <td>1.048332</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.013000</td>\n      <td>1.030326</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.619600</td>\n      <td>1.012292</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.870200</td>\n      <td>0.995531</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.181900</td>\n      <td>0.978182</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.222800</td>\n      <td>0.968035</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.872200</td>\n      <td>0.955171</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.851900</td>\n      <td>0.945949</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.693700</td>\n      <td>0.937941</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>1.349300</td>\n      <td>0.925129</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.021000</td>\n      <td>0.915483</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.181600</td>\n      <td>0.904909</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.023200</td>\n      <td>0.896457</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.839000</td>\n      <td>0.889046</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.711900</td>\n      <td>0.876341</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.887600</td>\n      <td>0.872190</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.839200</td>\n      <td>0.862671</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.898400</td>\n      <td>0.853245</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.711500</td>\n      <td>0.847319</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>1.089000</td>\n      <td>0.842428</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>1.240600</td>\n      <td>0.833003</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.706200</td>\n      <td>0.829644</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.946300</td>\n      <td>0.822630</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.519000</td>\n      <td>0.818867</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.715200</td>\n      <td>0.811056</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.760300</td>\n      <td>0.804845</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.613800</td>\n      <td>0.802271</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.982700</td>\n      <td>0.796571</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>1.071300</td>\n      <td>0.791237</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.813000</td>\n      <td>0.786493</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.679600</td>\n      <td>0.779546</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.079400</td>\n      <td>0.774443</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.589600</td>\n      <td>0.772519</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.571900</td>\n      <td>0.769412</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.472500</td>\n      <td>0.766276</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.797600</td>\n      <td>0.760942</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.775900</td>\n      <td>0.759186</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.859700</td>\n      <td>0.755682</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.858700</td>\n      <td>0.754077</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.931100</td>\n      <td>0.750828</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.677100</td>\n      <td>0.747782</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.701400</td>\n      <td>0.745449</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.598700</td>\n      <td>0.742703</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.631100</td>\n      <td>0.740895</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.764600</td>\n      <td>0.738660</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.933500</td>\n      <td>0.737360</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.042700</td>\n      <td>0.735807</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.527300</td>\n      <td>0.735097</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.438600</td>\n      <td>0.734531</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"messages = [\n    {'role': 'system', 'content': 'B·∫°n l√† chuy√™n gia gi√†u kinh nghi·ªám trong lƒ©nh v·ª±c d√¢n s·ª± v√† r·∫•t am hi·ªÉu v·ªÅ lu·∫≠t giao th√¥ng', 'thinking': None},\n    {\"role\": \"user\", \"content\": \"‚ÄúD√πng b·∫±ng l√°i xe √¥ t√¥ gi·∫£ b·ªã ph·∫°t bao nhi√™u 2025? \"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt = True,\n    return_tensors = \"pt\",\n    return_dict = True,\n    reasoning_effort = \"low\",\n).to(model.device)\nfrom transformers import TextStreamer\n_ = model.generate(**inputs, max_new_tokens = 512, streamer = TextStreamer(tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T15:18:45.503990Z","iopub.execute_input":"2025-12-14T15:18:45.504218Z","iopub.status.idle":"2025-12-14T15:19:09.684910Z","shell.execute_reply.started":"2025-12-14T15:18:45.504202Z","shell.execute_reply":"2025-12-14T15:19:09.684155Z"}},"outputs":[{"name":"stdout","text":"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 14 Dec 2025\n\nB·∫°n l√† chuy√™n gia gi√†u kinh nghi·ªám trong lƒ©nh v·ª±c d√¢n s·ª± v√† r·∫•t am hi·ªÉu v·ªÅ lu·∫≠t giao th√¥ng<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n‚ÄúD√πng b·∫±ng l√°i xe √¥ t√¥ gi·∫£ b·ªã ph·∫°t bao nhi√™u 2025?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nCƒÉn c·ª© theo ƒëi·ªÉm a kho·∫£n 5 ƒêi·ªÅu 28Ngh·ªã ƒë·ªãnh 168/2024/Nƒê-CPquy ƒë·ªãnh nh∆∞ sau:\nƒêi·ªÅu 28. X·ª≠ ph·∫°t ng∆∞·ªùi ƒëi·ªÅu khi·ªÉn xe √¥ t√¥ v√† c√°c lo·∫°i xe t∆∞∆°ng t·ª± xe √¥ t√¥ vi ph·∫°m quy ƒë·ªãnh v·ªÅ ƒëi·ªÅu ki·ªán c·ªßa ph∆∞∆°ng ti·ªán khi tham gia giao th√¥ng\n[...]\n5. Ph·∫°t ti·ªÅn t·ª´ 4.000.000 ƒë·ªìng ƒë·∫øn 6.000.000 ƒë·ªìng ƒë·ªëi v·ªõi ng∆∞·ªùi ƒëi·ªÅu khi·ªÉn xe th·ª±c hi·ªán m·ªôt trong c√°c h√†nh vi vi ph·∫°m sau ƒë√¢y:\na) S·ª≠ d·ª•ng Gi·∫•y ph√©p l√°i xe kh√¥ng do c∆° quan c√≥ th·∫©m quy·ªÅn c·∫•p (ho·∫∑c c·∫•p kh√¥ng ƒë√∫ng th·ªùi h·∫°n, h·∫øt h·∫°n s·ª≠ d·ª•ng), Gi·∫•y ph√©p l√°i xe b·ªã t·∫©y x√≥a, Gi·∫•y ph√©p l√°i xe kh√¥ng c√≤n hi·ªáu l·ª±c, Gi·∫•y ph√©p l√°i xe kh√¥ng h·ª£p l·ªá ho·∫∑c s·ª≠ d·ª•ng Gi·∫•y ph√©p l√°i xe kh√¥ng h·ª£p l·ªá;\n[...]\n10. Ngo√†i vi·ªác b·ªã ph·∫°t ti·ªÅn, ng∆∞·ªùi ƒëi·ªÅu khi·ªÉn ph∆∞∆°ng ti·ªán th·ª±c hi·ªán h√†nh vi vi ph·∫°m c√≤n b·ªã √°p d·ª•ng c√°c h√¨nh th·ª©c x·ª≠ ph·∫°t b·ªï sung sau ƒë√¢y:\na) Th·ª±c hi·ªán h√†nh vi quy ƒë·ªãnh t·∫°i ƒëi·ªÉm a kho·∫£n 3 ƒêi·ªÅu n√†y b·ªã t∆∞·ªõc quy·ªÅn s·ª≠ d·ª•ng Gi·∫•y ph√©p l√°i xe t·ª´ 10 th√°ng ƒë·∫øn 12 th√°ng;\nb) Th·ª±c hi·ªán h√†nh vi quy ƒë·ªãnh t·∫°i ƒëi·ªÉm a kho·∫£n 5 ƒêi·ªÅu n√†y b·ªã t∆∞·ªõc quy·ªÅn s·ª≠ d·ª•ng Gi·∫•y ph√©p l√°i xe t·ª´ 18 th√°ng ƒë·∫øn 20 th√°ng.\n[...]\nNh∆∞ v·∫≠y, nƒÉm 2025 ng∆∞·ªùi c√≥ h√†nh vi s·ª≠ d·ª•ng Gi·∫•y ph√©p l√°i xe kh√¥ng do c∆° quan c√≥ th·∫©m quy·ªÅn c·∫•p (ho·∫∑c c·∫•p kh√¥ng ƒë√∫ng th·ªùi h·∫°n, h·∫øt h·∫°n s·ª≠ d·ª•ng), Gi·∫•y ph√©p l√°i xe b·ªã t·∫©y x√≥a, Gi·∫•y ph√©p l√°i xe kh√¥ng c√≤n hi·ªáu l·ª±c, Gi·∫•y ph√©p l√°i xe kh√¥ng h·ª£p l·ªá ho·∫∑c s·ª≠ d·ª•ng Gi·∫•y ph√©p l√°i xe kh√¥ng h·ª£p l·ªá th√¨ s·∫Ω b·ªã ph·∫°t ti·ªÅn t·ª´ 4.000.000 ƒë·ªìng ƒë·∫øn 6.000.000 ƒë·ªìng.\nNgo√†i vi·ªác b·ªã ph·∫°t ti·ªÅn, ng∆∞·ªùi c√≥ h√†nh vi vi ph·∫°m n√†y c√≤n b·ªã t∆∞·ªõc quy·ªÅn s·ª≠ d·ª•ng Gi·∫•y ph√©p l√°i xe t·ª´ 10 th√°ng ƒë·∫øn 12 th√°ng.<|eot_id|>\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"model.save_pretrained(\"finetuned_model\")\ntokenizer.save_pretrained(\"finetuned_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:15:56.559794Z","iopub.execute_input":"2025-12-14T16:15:56.560448Z","iopub.status.idle":"2025-12-14T16:15:57.136665Z","shell.execute_reply.started":"2025-12-14T16:15:56.560423Z","shell.execute_reply":"2025-12-14T16:15:57.135966Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"('finetuned_model/tokenizer_config.json',\n 'finetuned_model/special_tokens_map.json',\n 'finetuned_model/chat_template.jinja',\n 'finetuned_model/tokenizer.json')"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"if False:\n  from unsloth import FastLanguageModel\n  model, tokenizer = FastLanguageModel.from_pretrained(\n      model_name = \"finetuned_model\", # YOUR MODEL YOU USED FOR TRAINING\n      max_seq_length = 1024,\n      dtype = None,\n      load_in_4bit = True,\n  )\n\nmessages = [\n    {'role': 'system', 'content': 'B·∫°n l√† chuy√™n gia gi√†u kinh nghi·ªám trong lƒ©nh v·ª±c d√¢n s·ª± v√† r·∫•t am hi·ªÉu v·ªÅ lu·∫≠t giao th√¥ng', 'thinking': None},\n    {\"role\": \"user\", \"content\": \"‚ÄúD√πng b·∫±ng l√°i xe √¥ t√¥ gi·∫£ b·ªã ph·∫°t bao nhi√™u 2025? \"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt = True,\n    return_tensors = \"pt\",\n    return_dict = True,\n    reasoning_effort = \"low\",\n).to(model.device)\nfrom transformers import TextStreamer\n_ = model.generate(**inputs, max_new_tokens = 512, streamer = TextStreamer(tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T15:19:10.055491Z","iopub.execute_input":"2025-12-14T15:19:10.055710Z","iopub.status.idle":"2025-12-14T15:19:31.324505Z","shell.execute_reply.started":"2025-12-14T15:19:10.055694Z","shell.execute_reply":"2025-12-14T15:19:31.323988Z"}},"outputs":[{"name":"stdout","text":"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 14 Dec 2025\n\nB·∫°n l√† chuy√™n gia gi√†u kinh nghi·ªám trong lƒ©nh v·ª±c d√¢n s·ª± v√† r·∫•t am hi·ªÉu v·ªÅ lu·∫≠t giao th√¥ng<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n‚ÄúD√πng b·∫±ng l√°i xe √¥ t√¥ gi·∫£ b·ªã ph·∫°t bao nhi√™u 2025?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nCƒÉn c·ª© kho·∫£n 3 ƒêi·ªÅu 15Ngh·ªã ƒë·ªãnh 168/2024/Nƒê-CPquy ƒë·ªãnh nh∆∞ sau:\nƒêi·ªÅu 15. X·ª≠ ph·∫°t, tr·ª´ ƒëi·ªÉm gi·∫•y ph√©p l√°i xe ƒë·ªëi v·ªõi ch·ªß ph∆∞∆°ng ti·ªán, ng∆∞·ªùi ƒëi·ªÅu khi·ªÉn ph∆∞∆°ng ti·ªán vi ph·∫°m quy ƒë·ªãnh v·ªÅ ƒëi·ªÅu ki·ªán c·ªßa ng∆∞·ªùi ƒëi·ªÅu khi·ªÉn ph∆∞∆°ng ti·ªán khi tham gia giao th√¥ng\n[...]\n3. Ph·∫°t ti·ªÅn t·ª´ 2.000.000 ƒë·ªìng ƒë·∫øn 3.000.000 ƒë·ªìng ƒë·ªëi v·ªõi m·ªôt trong c√°c h√†nh vi vi ph·∫°m sau ƒë√¢y:\na) D√πng b·∫±ng l√°i xe √¥ t√¥ gi·∫£;\nb) D√πng b·∫±ng l√°i xe √¥ t√¥ kh√¥ng ph√π h·ª£p v·ªõi lo·∫°i xe ƒëang ƒëi·ªÅu khi·ªÉn;\nc) D√πng b·∫±ng l√°i xe √¥ t√¥ kh√¥ng c√≥ gi·∫•y ph√©p l√°i xe ho·∫∑c s·ª≠ d·ª•ng gi·∫•y ph√©p l√°i xe kh√¥ng do c∆° quan c√≥ th·∫©m quy·ªÅn c·∫•p, gi·∫•y ph√©p l√°i xe ƒë√£ h·∫øt h·∫°n s·ª≠ d·ª•ng, gi·∫•y ph√©p l√°i xe b·ªã t·∫©y x√≥a, gi·∫•y ph√©p l√°i xe kh√¥ng c√≤n hi·ªáu l·ª±c;\nd) D√πng b·∫±ng l√°i xe √¥ t√¥ kh√¥ng c√≥ ch·ª©ng nh·∫≠n ƒëƒÉng k√Ω xe ho·∫∑c s·ª≠ d·ª•ng ch·ª©ng nh·∫≠n ƒëƒÉng k√Ω xe kh√¥ng do c∆° quan c√≥ th·∫©m quy·ªÅn c·∫•p, ch·ª©ng nh·∫≠n ƒëƒÉng k√Ω xe ƒë√£ h·∫øt h·∫°n s·ª≠ d·ª•ng, ch·ª©ng nh·∫≠n ƒëƒÉng k√Ω xe b·ªã t·∫©y x√≥a, ch·ª©ng nh·∫≠n ƒëƒÉng k√Ω xe kh√¥ng c√≤n hi·ªáu l·ª±c.\n[...]\nNh∆∞ v·∫≠y, ‚Äúd√πng b·∫±ng l√°i xe √¥ t√¥ gi·∫£‚Äù s·∫Ω b·ªã ph·∫°t ti·ªÅn t·ª´ 2.000.000 ƒë·ªìng ƒë·∫øn 3.000.000 ƒë·ªìng.\nL∆∞u √Ω: M·ª©c ph·∫°t ti·ªÅn quy ƒë·ªãnh t·∫°iNgh·ªã ƒë·ªãnh 168/2024/Nƒê-CPkh√¥ng √°p d·ª•ng ƒë·ªëi v·ªõi c√°c h√†nh vi vi ph·∫°m h√†nh ch√≠nh trong lƒ©nh v·ª±c giao th√¥ng ƒë∆∞·ªùng b·ªô, tr·ª´ c√°c h√†nh vi vi ph·∫°m h√†nh ch√≠nh quy ƒë·ªãnh t·∫°i ƒêi·ªÅu 3Ngh·ªã ƒë·ªãnh 168/2024/Nƒê-CP.<|eot_id|>\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"if False:\n  from unsloth import FastLanguageModel\n  model, tokenizer = FastLanguageModel.from_pretrained(\n      model_name = \"finetuned_model\", # YOUR MODEL YOU USED FOR TRAINING\n      max_seq_length = 1024,\n      dtype = None,\n      load_in_4bit = True,\n  )\n\nmessages = [\n    {'role': 'system', 'content': 'B·∫°n l√† chuy√™n gia gi√†u kinh nghi·ªám trong lƒ©nh v·ª±c d√¢n s·ª± v√† r·∫•t am hi·ªÉu v·ªÅ lu·∫≠t giao th√¥ng', 'thinking': None},\n    {\"role\": \"user\", \"content\": \"‚ÄúD√πng b·∫±ng l√°i xe √¥ t√¥ gi·∫£ b·ªã ph·∫°t bao nhi√™u 2025? \"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt = True,\n    return_tensors = \"pt\",\n    return_dict = True,\n    reasoning_effort = \"low\",\n).to(model.device)\nfrom transformers import TextStreamer\n_ = model.generate(**inputs, max_new_tokens = 512, streamer = TextStreamer(tokenizer))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### T·∫£i model v·ªÅ","metadata":{}},{"cell_type":"code","source":"merged_dir = \"/kaggle/working/finetuned_model\"\nmodel.save_pretrained_merged(merged_dir, tokenizer, save_method=\"merged_16bit\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T17:04:27.045978Z","iopub.execute_input":"2025-12-14T17:04:27.046722Z","iopub.status.idle":"2025-12-14T17:05:36.394769Z","shell.execute_reply.started":"2025-12-14T17:04:27.046695Z","shell.execute_reply":"2025-12-14T17:05:36.393826Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/890 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fb09c7995ac4302b496b265966f539a"}},"metadata":{}},{"name":"stdout","text":"Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba610bbbfcab4657bc78cd808682fbf7"}},"metadata":{}},{"name":"stdout","text":"Checking cache directory for required files...\nCache check failed: model-00001-of-00002.safetensors not found in local cache.\nNot all required files found in cache. Will proceed with downloading.\nChecking cache directory for required files...\nCache check failed: tokenizer.model not found in local cache.\nNot all required files found in cache. Will proceed with downloading.\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Preparing safetensor model files:   0%|          | 0/2 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d234ce80c5b45bea81aa5024876c660"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Preparing safetensor model files:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:21<00:21, 21.91s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"951f9f9828d947b987d4058a4b6ae8cb"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:26<00:00, 13.26s/it]\n","output_type":"stream"},{"name":"stdout","text":"Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:40<00:00, 20.23s/it]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merge process complete. Saved to `/kaggle/working/finetuned_model`\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}